
wcsa-ef-ingest.process-ef-json-mode = per-volume
#wcsa-ef-ingest.process-ef-json-mode = per-page
wcsa-ef-ingest.use-whitelist = true
wcsa-ef-ingest.whitelist-filename = file:/hdfsd05/dbbridge/whitelist-peter1.txt
#wcsa-ef-ingest.whitelist-filename = file:///home/dbbridge/extracted-features-solr/solr-ingest/whitelist-peter1.txt


#wcsa-ef-ingest.solr-clode-nodes = 10.11.0.53:8983,10.11.0.54:8983,10.11.0.55:8983
wcsa-ef-ingest.solr-cloud-nodes = gc0:8983,gc1:8983,gc2:8983,gc3:8983,gc4:8983,gc5:8983,gc6:8983,gc7:8983,gc8:8983,gc9:8983
wcsa-ef-ingest.icu-tokenize = true
wcsa-ef-ingest.strict-file-io = false


# For guide on number of partitions to use, see "Parallelized collections" section of:
#   https://spark.apache.org/docs/2.0.1/programming-guide.html
# which suggests 2-4 * num_cores
#
# For a more detailed discussion see:
#   http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/
	
# wcsa-ef-ingest.num-partitions = 12
#wcsa-ef-ingest.num-partitions = 110
#wcsa-ef-ingest.num-partitions = 220
#wcsa-ef-ingest.num-partitions = 400
#wcsa-ef-ingest.num-partitions = 1000
wcsa-ef-ingest.files-per-partition = 1300

spark.executor.cores=10

spark.driver.memory=50g
spark.executor.memory=70g
##spark.network.timeout=240s

#spark.local.dir=/var/tmp
