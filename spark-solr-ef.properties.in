
wcsa-ef-ingest.process-ef-json-mode = per-volume
#wcsa-ef-ingest.process-ef-json-mode = per-page
wcsa-ef-ingest.use-whitelist = true
wcsa-ef-ingest.whitelist-filename = file:/home/dbbridge/extracted-features-solr/solr-ingest/whitelist-peter1.txt
#wcsa-ef-ingest.whitelist-filename = file:/hdfsd05/dbbridge/whitelist-peter1.txt
#wcsa-ef-ingest.whitelist-filename = file:///home/dbbridge/extracted-features-solr/solr-ingest/whitelist-peter1.txt

wcsa-ef-ingest.use-langmap = true
wcsa-ef-ingest.langmap-directory = file:/home/dbbridge/extracted-features-solr/solr-ingest/opennlp-lang-pos-mappings/
#wcsa-ef-ingest.langmap-directory = hdfs://user/dbbridge/opennlp-lang-pos-mappings/

#wcsa-ef-ingest.solr-clode-nodes = 10.11.0.53:8983,10.11.0.54:8983,10.11.0.55:8983
#wcsa-ef-ingest.solr-cloud-nodes = gc0:8983,gc1:8983,gc2:8983,gc3:8983,gc4:8983,gc5:8983,gc6:8983,gc7:8983,gc8:8983,gc9:8983
#wcsa-ef-ingest.solr-cloud-nodes = solr1-s:8983,solr1-s:8984,solr1-s:8985,solr1-s:8986,solr1-s:8987,solr2-s:8983,solr2-s:8984,solr2-s:8985,solr2-s:8986,solr2-s:8987

####
# If we don't specify any 'solr-cloud-nodes' then is will use the solr-base-url argument provided by BASH script
####
#
#wcsa-ef-ingest.solr-cloud-nodes = 192.168.64.66:8983
#wcsa-ef-ingest.solr-cloud-nodes = gc0:8983,gc1:8983,gc2:8983,gc3:8983,gc4:8983,gc5:8983,gc6:8983,gc7:8983,gc8:8983,gc9:8983
#wcsa-ef-ingest.solr-cloud-nodes = solr1-s:8983,solr1-s:8984,solr1-s:8985,solr1-s:8986,solr1-s:8987,solr2-s:8983,solr2-s:8984,solr2-s:8985,solr2-s:8986,solr2-s:8987

# SOLR12 Solr8 
wcsa-ef-ingest.solr-cloud-nodes = solr1-s:9983,solr1-s:9984,solr1-s:9985,solr1-s:9986,solr1-s:9987,solr1-s:9988,solr1-s:9989,solr1-s:9990,solr1-s:9991,solr1-s:9992,solr2-s:9983,solr2-s:9984,solr2-s:9985,solr2-s:9986,solr2-s:9987,solr2-s:9988,solr2-s:9989,solr2-s:9990,solr2-s:9991,solr2-s:9992


wcsa-ef-ingest.icu-tokenize = true
wcsa-ef-ingest.strict-file-io = false


# For guide on number of partitions to use, see "Parallelized collections" section of:
#   https://spark.apache.org/docs/2.0.1/programming-guide.html
# which suggests 2-4 * num_cores
#
# For a more detailed discussion see:
#   http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/
	
# wcsa-ef-ingest.num-partitions = 12
#wcsa-ef-ingest.num-partitions = 110
#wcsa-ef-ingest.num-partitions = 220
#wcsa-ef-ingest.num-partitions = 400
#wcsa-ef-ingest.num-partitions = 1000

#wcsa-ef-ingest.files-per-partition = 1300
wcsa-ef-ingest.files-per-partition = 3000

#spark.executor.cores=10
#spark.executor.cores=2

#spark.driver.memory=50g
##spark.executor.memory=90g
###spark.network.timeout=240s
spark.network.timeout=360s

#spark.memory.fraction=0.2

##spark.shuffle.memoryFraction 0

# the following didn't seem to work
#spark.local.dir=/var/tmp
